{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Finetuning-BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvY6R2x3ESIq",
        "outputId": "eecd4585-fcb8-4149-c53d-c80669221785"
      },
      "source": [
        "# Install NVIDIA drivers\n",
        "!git clone https://github.com/NVIDIA/apex\n",
        "% cd apex\n",
        "!pip install -v --disable-pip-version-check --no-cache-dir ./\n",
        "% cd .."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 37, done.\u001b[K\n",
            "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 7872 (delta 10), reused 7 (delta 0), pack-reused 7835\u001b[K\n",
            "Receiving objects: 100% (7872/7872), 13.99 MiB | 27.04 MiB/s, done.\n",
            "Resolving deltas: 100% (5357/5357), done.\n",
            "/content/apex\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-jbwzbdb2\n",
            "Created temporary directory: /tmp/pip-req-tracker-zkl6q424\n",
            "Created requirements tracker '/tmp/pip-req-tracker-zkl6q424'\n",
            "Created temporary directory: /tmp/pip-install-7msi1opx\n",
            "Processing /content/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-cji__zbi\n",
            "  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-zkl6q424'\n",
            "    Running setup.py (path:/tmp/pip-req-build-cji__zbi/setup.py) egg_info for package from file:///content/apex\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.7.0+cu101\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-req-build-cji__zbi/pip-egg-info/apex.egg-info\n",
            "    writing /tmp/pip-req-build-cji__zbi/pip-egg-info/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-req-build-cji__zbi/pip-egg-info/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-req-build-cji__zbi/pip-egg-info/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-req-build-cji__zbi/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file '/tmp/pip-req-build-cji__zbi/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    /tmp/pip-req-build-cji__zbi/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  Source in /tmp/pip-req-build-cji__zbi has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n",
            "  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-zkl6q424'\n",
            "Building wheels for collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-wheel-s53pbhq8\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l  Destination directory: /tmp/pip-wheel-s53pbhq8\n",
            "  Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-cji__zbi/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-cji__zbi/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-s53pbhq8 --python-tag cp36\n",
            "\n",
            "\n",
            "  torch.__version__  = 1.7.0+cu101\n",
            "\n",
            "\n",
            "  /tmp/pip-req-build-cji__zbi/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "    warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib\n",
            "  creating build/lib/apex\n",
            "  copying apex/__init__.py -> build/lib/apex\n",
            "  creating build/lib/apex/mlp\n",
            "  copying apex/mlp/__init__.py -> build/lib/apex/mlp\n",
            "  copying apex/mlp/mlp.py -> build/lib/apex/mlp\n",
            "  creating build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/__init__.py -> build/lib/apex/multi_tensor_apply\n",
            "  creating build/lib/apex/RNN\n",
            "  copying apex/RNN/models.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/__init__.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/cells.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/RNNBackend.py -> build/lib/apex/RNN\n",
            "  creating build/lib/apex/contrib\n",
            "  copying apex/contrib/__init__.py -> build/lib/apex/contrib\n",
            "  creating build/lib/apex/parallel\n",
            "  copying apex/parallel/distributed.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/__init__.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/LARC.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/multiproc.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  creating build/lib/apex/normalization\n",
            "  copying apex/normalization/__init__.py -> build/lib/apex/normalization\n",
            "  copying apex/normalization/fused_layer_norm.py -> build/lib/apex/normalization\n",
            "  creating build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/__init__.py -> build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/weight_norm.py -> build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/reparameterization.py -> build/lib/apex/reparameterization\n",
            "  creating build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/__init__.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/loss_scaler.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16util.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16_optimizer.py -> build/lib/apex/fp16_utils\n",
            "  creating build/lib/apex/amp\n",
            "  copying apex/amp/__init__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_initialize.py -> build/lib/apex/amp\n",
            "  copying apex/amp/rnn_compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/opt.py -> build/lib/apex/amp\n",
            "  copying apex/amp/amp.py -> build/lib/apex/amp\n",
            "  copying apex/amp/scaler.py -> build/lib/apex/amp\n",
            "  copying apex/amp/frontend.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__version__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_amp_state.py -> build/lib/apex/amp\n",
            "  copying apex/amp/wrap.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_process_optimizer.py -> build/lib/apex/amp\n",
            "  copying apex/amp/handle.py -> build/lib/apex/amp\n",
            "  copying apex/amp/utils.py -> build/lib/apex/amp\n",
            "  copying apex/amp/compat.py -> build/lib/apex/amp\n",
            "  creating build/lib/apex/pyprof\n",
            "  copying apex/pyprof/__init__.py -> build/lib/apex/pyprof\n",
            "  creating build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_sgd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adam.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/__init__.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_novograd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_lamb.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adagrad.py -> build/lib/apex/optimizers\n",
            "  creating build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/__init__.py -> build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/batch_norm.py -> build/lib/apex/contrib/groupbn\n",
            "  creating build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/__init__.py -> build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib/apex/contrib/xentropy\n",
            "  creating build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/__init__.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  creating build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_sgd.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/__init__.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib/apex/contrib/optimizers\n",
            "  creating build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/sparse_masklib.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/__init__.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/asp.py -> build/lib/apex/contrib/sparsity\n",
            "  creating build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/tensor_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/__init__.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/torch_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/functional_overrides.py -> build/lib/apex/amp/lists\n",
            "  creating build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/reduction.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/convert.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/__init__.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/randomSample.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/pooling.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/linear.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/data.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/utility.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/recurrentCell.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/softmax.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/activation.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/usage.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/conv.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/__main__.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/base.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/optim.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/normalization.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/blas.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/misc.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/pointwise.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/loss.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/output.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/prof.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/dropout.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/embedding.py -> build/lib/apex/pyprof/prof\n",
            "  creating build/lib/apex/pyprof/nvtx\n",
            "  copying apex/pyprof/nvtx/__init__.py -> build/lib/apex/pyprof/nvtx\n",
            "  copying apex/pyprof/nvtx/nvmarker.py -> build/lib/apex/pyprof/nvtx\n",
            "  creating build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/parse.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/__init__.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/db.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/__main__.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/kernel.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/nvvp.py -> build/lib/apex/pyprof/parse\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/mlp.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/__init__.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/models.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/__init__.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/cells.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/distributed.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/LARC.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/weight_norm.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/reparameterization.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_initialize.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/opt.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/amp.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/scaler.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/frontend.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__version__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/wrap.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/handle.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/utils.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof\n",
            "  copying build/lib/apex/pyprof/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/reduction.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/convert.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/randomSample.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/pooling.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/linear.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/data.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/utility.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/recurrentCell.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/softmax.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/activation.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/usage.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/conv.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/__main__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/base.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/optim.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/normalization.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/blas.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/misc.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/pointwise.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/loss.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/output.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/index_slice_join_mutate.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/prof.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/dropout.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/embedding.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  copying build/lib/apex/pyprof/nvtx/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  copying build/lib/apex/pyprof/nvtx/nvmarker.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/parse.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/db.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/__main__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/kernel.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/nvvp.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  creating apex.egg-info\n",
            "  writing apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "  writing top-level names to apex.egg-info/top_level.txt\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  Copying apex.egg-info to build/bdist.linux-x86_64/wheel/apex-0.1-py3.6.egg-info\n",
            "  running install_scripts\n",
            "  adding license file \"LICENSE\" (matched pattern \"LICEN[CS]E*\")\n",
            "  creating build/bdist.linux-x86_64/wheel/apex-0.1.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-s53pbhq8/apex-0.1-cp36-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'apex/__init__.py'\n",
            "  adding 'apex/RNN/RNNBackend.py'\n",
            "  adding 'apex/RNN/__init__.py'\n",
            "  adding 'apex/RNN/cells.py'\n",
            "  adding 'apex/RNN/models.py'\n",
            "  adding 'apex/amp/__init__.py'\n",
            "  adding 'apex/amp/__version__.py'\n",
            "  adding 'apex/amp/_amp_state.py'\n",
            "  adding 'apex/amp/_initialize.py'\n",
            "  adding 'apex/amp/_process_optimizer.py'\n",
            "  adding 'apex/amp/amp.py'\n",
            "  adding 'apex/amp/compat.py'\n",
            "  adding 'apex/amp/frontend.py'\n",
            "  adding 'apex/amp/handle.py'\n",
            "  adding 'apex/amp/opt.py'\n",
            "  adding 'apex/amp/rnn_compat.py'\n",
            "  adding 'apex/amp/scaler.py'\n",
            "  adding 'apex/amp/utils.py'\n",
            "  adding 'apex/amp/wrap.py'\n",
            "  adding 'apex/amp/lists/__init__.py'\n",
            "  adding 'apex/amp/lists/functional_overrides.py'\n",
            "  adding 'apex/amp/lists/tensor_overrides.py'\n",
            "  adding 'apex/amp/lists/torch_overrides.py'\n",
            "  adding 'apex/contrib/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/batch_norm.py'\n",
            "  adding 'apex/contrib/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/mask_softmax_dropout_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam_v2.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam_v3.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fp16_optimizer.py'\n",
            "  adding 'apex/contrib/optimizers/fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fused_sgd.py'\n",
            "  adding 'apex/contrib/sparsity/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/asp.py'\n",
            "  adding 'apex/contrib/sparsity/sparse_masklib.py'\n",
            "  adding 'apex/contrib/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/xentropy/softmax_xentropy.py'\n",
            "  adding 'apex/fp16_utils/__init__.py'\n",
            "  adding 'apex/fp16_utils/fp16_optimizer.py'\n",
            "  adding 'apex/fp16_utils/fp16util.py'\n",
            "  adding 'apex/fp16_utils/loss_scaler.py'\n",
            "  adding 'apex/mlp/__init__.py'\n",
            "  adding 'apex/mlp/mlp.py'\n",
            "  adding 'apex/multi_tensor_apply/__init__.py'\n",
            "  adding 'apex/multi_tensor_apply/multi_tensor_apply.py'\n",
            "  adding 'apex/normalization/__init__.py'\n",
            "  adding 'apex/normalization/fused_layer_norm.py'\n",
            "  adding 'apex/optimizers/__init__.py'\n",
            "  adding 'apex/optimizers/fused_adagrad.py'\n",
            "  adding 'apex/optimizers/fused_adam.py'\n",
            "  adding 'apex/optimizers/fused_lamb.py'\n",
            "  adding 'apex/optimizers/fused_novograd.py'\n",
            "  adding 'apex/optimizers/fused_sgd.py'\n",
            "  adding 'apex/parallel/LARC.py'\n",
            "  adding 'apex/parallel/__init__.py'\n",
            "  adding 'apex/parallel/distributed.py'\n",
            "  adding 'apex/parallel/multiproc.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm_kernel.py'\n",
            "  adding 'apex/parallel/sync_batchnorm.py'\n",
            "  adding 'apex/parallel/sync_batchnorm_kernel.py'\n",
            "  adding 'apex/pyprof/__init__.py'\n",
            "  adding 'apex/pyprof/nvtx/__init__.py'\n",
            "  adding 'apex/pyprof/nvtx/nvmarker.py'\n",
            "  adding 'apex/pyprof/parse/__init__.py'\n",
            "  adding 'apex/pyprof/parse/__main__.py'\n",
            "  adding 'apex/pyprof/parse/db.py'\n",
            "  adding 'apex/pyprof/parse/kernel.py'\n",
            "  adding 'apex/pyprof/parse/nvvp.py'\n",
            "  adding 'apex/pyprof/parse/parse.py'\n",
            "  adding 'apex/pyprof/prof/__init__.py'\n",
            "  adding 'apex/pyprof/prof/__main__.py'\n",
            "  adding 'apex/pyprof/prof/activation.py'\n",
            "  adding 'apex/pyprof/prof/base.py'\n",
            "  adding 'apex/pyprof/prof/blas.py'\n",
            "  adding 'apex/pyprof/prof/conv.py'\n",
            "  adding 'apex/pyprof/prof/convert.py'\n",
            "  adding 'apex/pyprof/prof/data.py'\n",
            "  adding 'apex/pyprof/prof/dropout.py'\n",
            "  adding 'apex/pyprof/prof/embedding.py'\n",
            "  adding 'apex/pyprof/prof/index_slice_join_mutate.py'\n",
            "  adding 'apex/pyprof/prof/linear.py'\n",
            "  adding 'apex/pyprof/prof/loss.py'\n",
            "  adding 'apex/pyprof/prof/misc.py'\n",
            "  adding 'apex/pyprof/prof/normalization.py'\n",
            "  adding 'apex/pyprof/prof/optim.py'\n",
            "  adding 'apex/pyprof/prof/output.py'\n",
            "  adding 'apex/pyprof/prof/pointwise.py'\n",
            "  adding 'apex/pyprof/prof/pooling.py'\n",
            "  adding 'apex/pyprof/prof/prof.py'\n",
            "  adding 'apex/pyprof/prof/randomSample.py'\n",
            "  adding 'apex/pyprof/prof/recurrentCell.py'\n",
            "  adding 'apex/pyprof/prof/reduction.py'\n",
            "  adding 'apex/pyprof/prof/softmax.py'\n",
            "  adding 'apex/pyprof/prof/usage.py'\n",
            "  adding 'apex/pyprof/prof/utility.py'\n",
            "  adding 'apex/reparameterization/__init__.py'\n",
            "  adding 'apex/reparameterization/reparameterization.py'\n",
            "  adding 'apex/reparameterization/weight_norm.py'\n",
            "  adding 'apex-0.1.dist-info/LICENSE'\n",
            "  adding 'apex-0.1.dist-info/METADATA'\n",
            "  adding 'apex-0.1.dist-info/WHEEL'\n",
            "  adding 'apex-0.1.dist-info/top_level.txt'\n",
            "  adding 'apex-0.1.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.1-cp36-none-any.whl size=194539 sha256=1023c709707fa7bb49f81ccce2f9bbfc0dc519add946dd7eaa695e4b9b414852\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jbwzbdb2/wheels/b1/3a/aa/d84906eaab780ae580c7a5686a33bf2820d8590ac3b60d5967\n",
            "  Removing source in /tmp/pip-req-build-cji__zbi\n",
            "Successfully built apex\n",
            "Installing collected packages: apex\n",
            "\n",
            "Successfully installed apex-0.1\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-zkl6q424'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJPrpWaREXWS",
        "outputId": "b8252817-269f-4644-a317-767a3b300fad"
      },
      "source": [
        "!pip install datasets transformers -q"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 163kB 14.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5MB 27.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 245kB 49.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 17.7MB 212kB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 44.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 50.7MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lk0LME9FRfQ",
        "outputId": "c3d8e9eb-ccc0-4ad8-fa98-55a1c110239a"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 62, done.\u001b[K\n",
            "remote: Counting objects: 100% (62/62), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 57442 (delta 19), reused 11 (delta 1), pack-reused 57380\u001b[K\n",
            "Receiving objects: 100% (57442/57442), 42.86 MiB | 27.14 MiB/s, done.\n",
            "Resolving deltas: 100% (40282/40282), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_jxLrE7FPYb"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import datasets\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjT08U8pZGte"
      },
      "source": [
        "## Loading the corpora"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O61OCVLyFSu4",
        "outputId": "4843224e-43a4-468c-d2c1-152396647539"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdbtyP0YFYEm"
      },
      "source": [
        "train_path = \"/content/gdrive/MyDrive/train.txt\"\n",
        "eval_path = \"/content/gdrive/MyDrive/eval.txt\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsriEpV9GF1O"
      },
      "source": [
        "model_dir = 'model'\n",
        "!mkdir {model_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv1dlZdQZ1jP"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGstCTDkXEZz"
      },
      "source": [
        "cmd ='''python /content/transformers/examples/language-modeling/run_mlm.py \\\n",
        "    --mlm 0.15 \\\n",
        "    --model_name_or_path bert-base-uncased \\\n",
        "    --train_file {} \\\n",
        "    --validation_file {} \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --output_dir {} \\\n",
        "    --fp16 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --max_seq_length 128 \\\n",
        "    --save_total_limit 2 \\\n",
        "    --save_steps 40000 \\\n",
        "    '''.format(train_path, eval_path, model_dir)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qi-kwufaAqC",
        "outputId": "a9f8b488-e2b1-47ed-956a-cbbcbcbf1739"
      },
      "source": [
        "! {cmd}"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-24 11:01:04.105206: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "12/24/2020 11:01:05 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "12/24/2020 11:01:05 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='model', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, model_parallel=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec24_11-01-05_eb2c441f5812', logging_first_step=False, logging_steps=500, save_steps=40000, save_total_limit=2, no_cuda=False, seed=42, fp16=True, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='model', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fp16_backend='auto', sharded_ddp=False)\n",
            "Downloading: 2.57kB [00:00, 3.32MB/s]       \n",
            "Using custom data configuration default\n",
            "Downloading and preparing dataset text/default-58e884c7ffa7af44 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-58e884c7ffa7af44/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab...\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-58e884c7ffa7af44/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab. Subsequent calls will reuse this data.\n",
            "12/24/2020 11:01:10 - INFO - filelock -   Lock 140182219255760 acquired on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.637c6035640bacb831febcc2b7f7bee0a96f9b30c2d7e9ef84082d9f252f3170.lock\n",
            "[INFO|file_utils.py:1301] 2020-12-24 11:01:10,575 >> https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp64ckqtto\n",
            "Downloading: 100% 433/433 [00:00<00:00, 590kB/s]\n",
            "[INFO|file_utils.py:1305] 2020-12-24 11:01:10,591 >> storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.637c6035640bacb831febcc2b7f7bee0a96f9b30c2d7e9ef84082d9f252f3170\n",
            "[INFO|file_utils.py:1308] 2020-12-24 11:01:10,591 >> creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.637c6035640bacb831febcc2b7f7bee0a96f9b30c2d7e9ef84082d9f252f3170\n",
            "12/24/2020 11:01:10 - INFO - filelock -   Lock 140182219255760 released on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.637c6035640bacb831febcc2b7f7bee0a96f9b30c2d7e9ef84082d9f252f3170.lock\n",
            "[INFO|configuration_utils.py:431] 2020-12-24 11:01:10,592 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.637c6035640bacb831febcc2b7f7bee0a96f9b30c2d7e9ef84082d9f252f3170\n",
            "[INFO|configuration_utils.py:467] 2020-12-24 11:01:10,593 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:431] 2020-12-24 11:01:10,609 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.637c6035640bacb831febcc2b7f7bee0a96f9b30c2d7e9ef84082d9f252f3170\n",
            "[INFO|configuration_utils.py:467] 2020-12-24 11:01:10,609 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "12/24/2020 11:01:10 - INFO - filelock -   Lock 140182163358608 acquired on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
            "[INFO|file_utils.py:1301] 2020-12-24 11:01:10,628 >> https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp9976wjmv\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 20.4MB/s]\n",
            "[INFO|file_utils.py:1305] 2020-12-24 11:01:10,657 >> storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|file_utils.py:1308] 2020-12-24 11:01:10,657 >> creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "12/24/2020 11:01:10 - INFO - filelock -   Lock 140182163358608 released on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
            "12/24/2020 11:01:10 - INFO - filelock -   Lock 140182163358608 acquired on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
            "[INFO|file_utils.py:1301] 2020-12-24 11:01:10,680 >> https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpvy6fe2k3\n",
            "Downloading: 100% 466k/466k [00:00<00:00, 28.2MB/s]\n",
            "[INFO|file_utils.py:1305] 2020-12-24 11:01:10,717 >> storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|file_utils.py:1308] 2020-12-24 11:01:10,717 >> creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "12/24/2020 11:01:10 - INFO - filelock -   Lock 140182163358608 released on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
            "[INFO|tokenization_utils_base.py:1802] 2020-12-24 11:01:10,717 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|tokenization_utils_base.py:1802] 2020-12-24 11:01:10,717 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "12/24/2020 11:01:10 - INFO - filelock -   Lock 140182163357880 acquired on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
            "[INFO|file_utils.py:1301] 2020-12-24 11:01:10,753 >> https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpj_lb7eeu\n",
            "Downloading: 100% 440M/440M [00:05<00:00, 81.4MB/s]\n",
            "[INFO|file_utils.py:1305] 2020-12-24 11:01:16,273 >> storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[INFO|file_utils.py:1308] 2020-12-24 11:01:16,274 >> creating metadata file for /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "12/24/2020 11:01:16 - INFO - filelock -   Lock 140182163357880 released on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
            "[INFO|modeling_utils.py:1024] 2020-12-24 11:01:16,274 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[WARNING|modeling_utils.py:1132] 2020-12-24 11:01:20,526 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[INFO|modeling_utils.py:1149] 2020-12-24 11:01:20,526 >> All the weights of BertForMaskedLM were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
            " 34% 1349/3983 [01:17<02:45, 15.93ba/s][WARNING|tokenization_utils_base.py:3233] 2020-12-24 11:02:37,738 >> Token indices sequence length is longer than the specified maximum sequence length for this model (576 > 512). Running this sequence through the model will result in indexing errors\n",
            "100% 3983/3983 [03:46<00:00, 17.61ba/s]\n",
            "100% 443/443 [00:24<00:00, 17.83ba/s]\n",
            "100% 3983/3983 [09:42<00:00,  6.84ba/s]\n",
            "100% 443/443 [01:04<00:00,  6.85ba/s]\n",
            "[INFO|trainer.py:388] 2020-12-24 11:16:33,209 >> The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[INFO|trainer.py:388] 2020-12-24 11:16:33,210 >> The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[INFO|trainer.py:703] 2020-12-24 11:16:33,214 >> ***** Running training *****\n",
            "[INFO|trainer.py:704] 2020-12-24 11:16:33,214 >>   Num examples = 499258\n",
            "[INFO|trainer.py:705] 2020-12-24 11:16:33,214 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:706] 2020-12-24 11:16:33,214 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:707] 2020-12-24 11:16:33,214 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:708] 2020-12-24 11:16:33,214 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:709] 2020-12-24 11:16:33,214 >>   Total optimization steps = 62408\n",
            "  0% 0/62408 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "{'loss': 2.181008056640625, 'learning_rate': 4.959941033200872e-05, 'epoch': 0.008011793359825664}\n",
            "{'loss': 2.0737529296875, 'learning_rate': 4.9198820664017434e-05, 'epoch': 0.016023586719651327}\n",
            "{'loss': 2.0417236328125, 'learning_rate': 4.879823099602615e-05, 'epoch': 0.02403538007947699}\n",
            "{'loss': 1.9872518310546874, 'learning_rate': 4.839764132803487e-05, 'epoch': 0.032047173439302655}\n",
            "{'loss': 1.970692626953125, 'learning_rate': 4.799705166004359e-05, 'epoch': 0.040058966799128314}\n",
            "{'loss': 1.97713916015625, 'learning_rate': 4.7596461992052304e-05, 'epoch': 0.04807076015895398}\n",
            "{'loss': 1.9366414794921876, 'learning_rate': 4.719587232406102e-05, 'epoch': 0.056082553518779645}\n",
            "{'loss': 1.9247183837890625, 'learning_rate': 4.679528265606974e-05, 'epoch': 0.06409434687860531}\n",
            "{'loss': 1.908912353515625, 'learning_rate': 4.639469298807845e-05, 'epoch': 0.07210614023843097}\n",
            "{'loss': 1.8937833251953125, 'learning_rate': 4.599410332008717e-05, 'epoch': 0.08011793359825663}\n",
            "{'loss': 1.8710787353515625, 'learning_rate': 4.559351365209589e-05, 'epoch': 0.0881297269580823}\n",
            "{'loss': 1.8680943603515625, 'learning_rate': 4.5192923984104605e-05, 'epoch': 0.09614152031790796}\n",
            "{'loss': 1.8585355224609375, 'learning_rate': 4.479233431611332e-05, 'epoch': 0.10415331367773363}\n",
            "{'loss': 1.8655948486328124, 'learning_rate': 4.4391744648122036e-05, 'epoch': 0.11216510703755929}\n",
            "{'loss': 1.8413275146484376, 'learning_rate': 4.399115498013076e-05, 'epoch': 0.12017690039738495}\n",
            "{'loss': 1.8571455078125, 'learning_rate': 4.359056531213947e-05, 'epoch': 0.12818869375721062}\n",
            "{'loss': 1.842154052734375, 'learning_rate': 4.318997564414819e-05, 'epoch': 0.13620048711703628}\n",
            "{'loss': 1.836781005859375, 'learning_rate': 4.2789385976156906e-05, 'epoch': 0.14421228047686194}\n",
            "{'loss': 1.8421844482421874, 'learning_rate': 4.238879630816562e-05, 'epoch': 0.1522240738366876}\n",
            "{'loss': 1.814408935546875, 'learning_rate': 4.198820664017434e-05, 'epoch': 0.16023586719651325}\n",
            "{'loss': 1.819947509765625, 'learning_rate': 4.158761697218306e-05, 'epoch': 0.16824766055633894}\n",
            "{'loss': 1.8103734130859375, 'learning_rate': 4.118702730419177e-05, 'epoch': 0.1762594539161646}\n",
            "{'loss': 1.80761669921875, 'learning_rate': 4.0786437636200484e-05, 'epoch': 0.18427124727599026}\n",
            "{'loss': 1.8154395751953125, 'learning_rate': 4.038584796820921e-05, 'epoch': 0.19228304063581592}\n",
            "{'loss': 1.77264453125, 'learning_rate': 3.998525830021792e-05, 'epoch': 0.20029483399564157}\n",
            "{'loss': 1.7975455322265625, 'learning_rate': 3.958466863222664e-05, 'epoch': 0.20830662735546726}\n",
            "{'loss': 1.796515625, 'learning_rate': 3.9184078964235354e-05, 'epoch': 0.21631842071529292}\n",
            "{'loss': 1.7903404541015624, 'learning_rate': 3.878348929624408e-05, 'epoch': 0.22433021407511858}\n",
            "{'loss': 1.7610709228515624, 'learning_rate': 3.8382899628252786e-05, 'epoch': 0.23234200743494424}\n",
            "{'loss': 1.7862958984375, 'learning_rate': 3.798230996026151e-05, 'epoch': 0.2403538007947699}\n",
            "{'loss': 1.764853515625, 'learning_rate': 3.7581720292270224e-05, 'epoch': 0.24836559415459555}\n",
            "{'loss': 1.7729114990234376, 'learning_rate': 3.718113062427894e-05, 'epoch': 0.25637738751442124}\n",
            "{'loss': 1.76179443359375, 'learning_rate': 3.6780540956287655e-05, 'epoch': 0.2643891808742469}\n",
            "{'loss': 1.737114990234375, 'learning_rate': 3.637995128829638e-05, 'epoch': 0.27240097423407256}\n",
            "{'loss': 1.7459346923828125, 'learning_rate': 3.5979361620305094e-05, 'epoch': 0.2804127675938982}\n",
            "{'loss': 1.77462353515625, 'learning_rate': 3.55787719523138e-05, 'epoch': 0.2884245609537239}\n",
            "{'loss': 1.7442969970703126, 'learning_rate': 3.5178182284322525e-05, 'epoch': 0.29643635431354953}\n",
            "{'loss': 1.743134765625, 'learning_rate': 3.477759261633124e-05, 'epoch': 0.3044481476733752}\n",
            "{'loss': 1.738540283203125, 'learning_rate': 3.4377002948339956e-05, 'epoch': 0.31245994103320085}\n",
            "{'loss': 1.7406068115234374, 'learning_rate': 3.397641328034867e-05, 'epoch': 0.3204717343930265}\n",
            "{'loss': 1.7449951171875, 'learning_rate': 3.3575823612357395e-05, 'epoch': 0.3284835277528522}\n",
            "{'loss': 1.7172183837890624, 'learning_rate': 3.317523394436611e-05, 'epoch': 0.3364953211126779}\n",
            "{'loss': 1.7189737548828126, 'learning_rate': 3.2774644276374826e-05, 'epoch': 0.34450711447250354}\n",
            "{'loss': 1.7299185791015625, 'learning_rate': 3.237405460838354e-05, 'epoch': 0.3525189078323292}\n",
            "{'loss': 1.735568359375, 'learning_rate': 3.1973464940392264e-05, 'epoch': 0.36053070119215486}\n",
            "{'loss': 1.73324853515625, 'learning_rate': 3.157287527240097e-05, 'epoch': 0.3685424945519805}\n",
            "{'loss': 1.7008355712890626, 'learning_rate': 3.117228560440969e-05, 'epoch': 0.3765542879118062}\n",
            "{'loss': 1.714650634765625, 'learning_rate': 3.077169593641841e-05, 'epoch': 0.38456608127163183}\n",
            "{'loss': 1.7207764892578126, 'learning_rate': 3.0371106268427124e-05, 'epoch': 0.3925778746314575}\n",
            "{'loss': 1.70247998046875, 'learning_rate': 2.9970516600435843e-05, 'epoch': 0.40058966799128315}\n",
            "{'loss': 1.72167724609375, 'learning_rate': 2.956992693244456e-05, 'epoch': 0.4086014613511088}\n",
            "{'loss': 1.707015380859375, 'learning_rate': 2.9169337264453278e-05, 'epoch': 0.4166132547109345}\n",
            "{'loss': 1.6963577880859375, 'learning_rate': 2.8768747596461994e-05, 'epoch': 0.4246250480707602}\n",
            "{'loss': 1.6921302490234376, 'learning_rate': 2.8368157928470713e-05, 'epoch': 0.43263684143058584}\n",
            "{'loss': 1.6756697998046874, 'learning_rate': 2.7967568260479425e-05, 'epoch': 0.4406486347904115}\n",
            "{'loss': 1.7040413818359375, 'learning_rate': 2.7566978592488148e-05, 'epoch': 0.44866042815023716}\n",
            "{'loss': 1.6752330322265625, 'learning_rate': 2.716638892449686e-05, 'epoch': 0.4566722215100628}\n",
            "{'loss': 1.6906077880859376, 'learning_rate': 2.6765799256505576e-05, 'epoch': 0.4646840148698885}\n",
            "{'loss': 1.6675701904296876, 'learning_rate': 2.6365209588514295e-05, 'epoch': 0.47269580822971413}\n",
            "{'loss': 1.6679111328125, 'learning_rate': 2.596461992052301e-05, 'epoch': 0.4807076015895398}\n",
            "{'loss': 1.701482421875, 'learning_rate': 2.556403025253173e-05, 'epoch': 0.48871939494936545}\n",
            "{'loss': 1.6586890869140625, 'learning_rate': 2.5163440584540442e-05, 'epoch': 0.4967311883091911}\n",
            "{'loss': 1.64986669921875, 'learning_rate': 2.4762850916549164e-05, 'epoch': 0.5047429816690168}\n",
            "{'loss': 1.6863145751953126, 'learning_rate': 2.4362261248557877e-05, 'epoch': 0.5127547750288425}\n",
            "{'loss': 1.6654649658203124, 'learning_rate': 2.3961671580566596e-05, 'epoch': 0.5207665683886681}\n",
            "{'loss': 1.649108154296875, 'learning_rate': 2.356108191257531e-05, 'epoch': 0.5287783617484938}\n",
            "{'loss': 1.6659278564453126, 'learning_rate': 2.3160492244584027e-05, 'epoch': 0.5367901551083194}\n",
            "{'loss': 1.64433154296875, 'learning_rate': 2.2759902576592746e-05, 'epoch': 0.5448019484681451}\n",
            "{'loss': 1.6499237060546874, 'learning_rate': 2.2359312908601462e-05, 'epoch': 0.5528137418279708}\n",
            "{'loss': 1.6451585693359374, 'learning_rate': 2.1958723240610178e-05, 'epoch': 0.5608255351877964}\n",
            "{'loss': 1.6497232666015624, 'learning_rate': 2.1558133572618897e-05, 'epoch': 0.5688373285476221}\n",
            "{'loss': 1.64071923828125, 'learning_rate': 2.1157543904627613e-05, 'epoch': 0.5768491219074477}\n",
            "{'loss': 1.626145751953125, 'learning_rate': 2.0756954236636332e-05, 'epoch': 0.5848609152672735}\n",
            "{'loss': 1.6249041748046875, 'learning_rate': 2.0356364568645048e-05, 'epoch': 0.5928727086270991}\n",
            "{'loss': 1.6394158935546874, 'learning_rate': 1.9955774900653763e-05, 'epoch': 0.6008845019869248}\n",
            "{'loss': 1.6407147216796876, 'learning_rate': 1.955518523266248e-05, 'epoch': 0.6088962953467504}\n",
            "{'loss': 1.6181771240234375, 'learning_rate': 1.9154595564671195e-05, 'epoch': 0.6169080887065761}\n",
            "{'loss': 1.6231944580078126, 'learning_rate': 1.8754005896679914e-05, 'epoch': 0.6249198820664017}\n",
            "{'loss': 1.6376846923828126, 'learning_rate': 1.835341622868863e-05, 'epoch': 0.6329316754262274}\n",
            "{'loss': 1.623345947265625, 'learning_rate': 1.795282656069735e-05, 'epoch': 0.640943468786053}\n",
            " 64% 40000/62408 [1:50:03<1:02:18,  5.99it/s][INFO|trainer.py:1226] 2020-12-24 13:06:36,706 >> Saving model checkpoint to model/checkpoint-40000\n",
            "[INFO|configuration_utils.py:289] 2020-12-24 13:06:36,708 >> Configuration saved in model/checkpoint-40000/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-24 13:06:38,224 >> Model weights saved in model/checkpoint-40000/pytorch_model.bin\n",
            "{'loss': 1.6160755615234375, 'learning_rate': 1.7552236892706064e-05, 'epoch': 0.6489552621458787}\n",
            "{'loss': 1.5995096435546876, 'learning_rate': 1.715164722471478e-05, 'epoch': 0.6569670555057044}\n",
            "{'loss': 1.610924560546875, 'learning_rate': 1.67510575567235e-05, 'epoch': 0.66497884886553}\n",
            "{'loss': 1.6171060791015626, 'learning_rate': 1.6350467888732215e-05, 'epoch': 0.6729906422253558}\n",
            "{'loss': 1.6075626220703125, 'learning_rate': 1.5949878220740934e-05, 'epoch': 0.6810024355851814}\n",
            "{'loss': 1.597594482421875, 'learning_rate': 1.554928855274965e-05, 'epoch': 0.6890142289450071}\n",
            "{'loss': 1.587355224609375, 'learning_rate': 1.5148698884758364e-05, 'epoch': 0.6970260223048327}\n",
            "{'loss': 1.60897021484375, 'learning_rate': 1.4748109216767081e-05, 'epoch': 0.7050378156646584}\n",
            "{'loss': 1.60409326171875, 'learning_rate': 1.4347519548775799e-05, 'epoch': 0.713049609024484}\n",
            "{'loss': 1.6077347412109375, 'learning_rate': 1.3946929880784515e-05, 'epoch': 0.7210614023843097}\n",
            "{'loss': 1.597002197265625, 'learning_rate': 1.3546340212793232e-05, 'epoch': 0.7290731957441353}\n",
            "{'loss': 1.5918143310546875, 'learning_rate': 1.314575054480195e-05, 'epoch': 0.737084989103961}\n",
            "{'loss': 1.588522216796875, 'learning_rate': 1.2745160876810667e-05, 'epoch': 0.7450967824637867}\n",
            "{'loss': 1.604143310546875, 'learning_rate': 1.2344571208819383e-05, 'epoch': 0.7531085758236123}\n",
            "{'loss': 1.601397705078125, 'learning_rate': 1.1943981540828098e-05, 'epoch': 0.7611203691834381}\n",
            "{'loss': 1.5808612060546876, 'learning_rate': 1.1543391872836816e-05, 'epoch': 0.7691321625432637}\n",
            "{'loss': 1.576986572265625, 'learning_rate': 1.1142802204845533e-05, 'epoch': 0.7771439559030894}\n",
            "{'loss': 1.559443603515625, 'learning_rate': 1.074221253685425e-05, 'epoch': 0.785155749262915}\n",
            "{'loss': 1.5877274169921876, 'learning_rate': 1.0341622868862968e-05, 'epoch': 0.7931675426227407}\n",
            "{'loss': 1.5698175048828125, 'learning_rate': 9.941033200871684e-06, 'epoch': 0.8011793359825663}\n",
            "{'loss': 1.559851318359375, 'learning_rate': 9.5404435328804e-06, 'epoch': 0.809191129342392}\n",
            "{'loss': 1.5651085205078126, 'learning_rate': 9.139853864889117e-06, 'epoch': 0.8172029227022176}\n",
            "{'loss': 1.5606163330078124, 'learning_rate': 8.739264196897834e-06, 'epoch': 0.8252147160620433}\n",
            "{'loss': 1.5887882080078124, 'learning_rate': 8.338674528906552e-06, 'epoch': 0.833226509421869}\n",
            "{'loss': 1.5667388916015625, 'learning_rate': 7.938084860915267e-06, 'epoch': 0.8412383027816946}\n",
            "{'loss': 1.5678277587890626, 'learning_rate': 7.537495192923985e-06, 'epoch': 0.8492500961415204}\n",
            "{'loss': 1.564276611328125, 'learning_rate': 7.1369055249327005e-06, 'epoch': 0.857261889501346}\n",
            "{'loss': 1.5831329345703125, 'learning_rate': 6.736315856941418e-06, 'epoch': 0.8652736828611717}\n",
            "{'loss': 1.5524580078125, 'learning_rate': 6.3357261889501345e-06, 'epoch': 0.8732854762209973}\n",
            "{'loss': 1.5429039306640624, 'learning_rate': 5.935136520958852e-06, 'epoch': 0.881297269580823}\n",
            "{'loss': 1.54739306640625, 'learning_rate': 5.5345468529675685e-06, 'epoch': 0.8893090629406486}\n",
            "{'loss': 1.55591552734375, 'learning_rate': 5.133957184976285e-06, 'epoch': 0.8973208563004743}\n",
            "{'loss': 1.5526875, 'learning_rate': 4.7333675169850025e-06, 'epoch': 0.9053326496602999}\n",
            "{'loss': 1.5716932373046875, 'learning_rate': 4.332777848993719e-06, 'epoch': 0.9133444430201256}\n",
            "{'loss': 1.5478162841796874, 'learning_rate': 3.932188181002436e-06, 'epoch': 0.9213562363799513}\n",
            "{'loss': 1.54585888671875, 'learning_rate': 3.5315985130111527e-06, 'epoch': 0.929368029739777}\n",
            "{'loss': 1.5491929931640624, 'learning_rate': 3.1310088450198692e-06, 'epoch': 0.9373798230996027}\n",
            "{'loss': 1.549841796875, 'learning_rate': 2.7304191770285862e-06, 'epoch': 0.9453916164594283}\n",
            "{'loss': 1.5530198974609375, 'learning_rate': 2.329829509037303e-06, 'epoch': 0.953403409819254}\n",
            "{'loss': 1.519111572265625, 'learning_rate': 1.92923984104602e-06, 'epoch': 0.9614152031790796}\n",
            "{'loss': 1.54720458984375, 'learning_rate': 1.5286501730547366e-06, 'epoch': 0.9694269965389053}\n",
            "{'loss': 1.54352978515625, 'learning_rate': 1.1280605050634534e-06, 'epoch': 0.9774387898987309}\n",
            "{'loss': 1.5428275146484376, 'learning_rate': 7.274708370721703e-07, 'epoch': 0.9854505832585566}\n",
            "{'loss': 1.5581646728515626, 'learning_rate': 3.2688116908088707e-07, 'epoch': 0.9934623766183822}\n",
            "100% 62408/62408 [2:51:28<00:00,  6.86it/s][INFO|trainer.py:862] 2020-12-24 14:08:02,231 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'epoch': 1.0}\n",
            "100% 62408/62408 [2:51:28<00:00,  6.07it/s]\n",
            "[INFO|trainer.py:1226] 2020-12-24 14:08:02,233 >> Saving model checkpoint to model\n",
            "[INFO|configuration_utils.py:289] 2020-12-24 14:08:02,234 >> Configuration saved in model/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-24 14:08:03,626 >> Model weights saved in model/pytorch_model.bin\n",
            "12/24/2020 14:08:03 - INFO - __main__ -   ***** Train results *****\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/transformers/examples/language-modeling/run_mlm.py\", line 420, in <module>\n",
            "    main()\n",
            "  File \"/content/transformers/examples/language-modeling/run_mlm.py\", line 386, in main\n",
            "    for key, value in sorted(train_result.metrics.items()):\n",
            "AttributeError: 'TrainOutput' object has no attribute 'metrics'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LukXEbtpaJUh"
      },
      "source": [
        ""
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tYtwuyRaGlI"
      },
      "source": [
        "## Loading the fine-tuned model and examining its performance on the MLM task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vjo3VJD9FHIV"
      },
      "source": [
        "trained_model = transformers.BertForMaskedLM.from_pretrained(model_dir)\n",
        "trained_tokenizer = transformers.BertTokenizer.from_pretrained(model_dir)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qCBLJQVEvBe"
      },
      "source": [
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model= trained_model,\n",
        "    tokenizer=trained_tokenizer\n",
        ")"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpAj9eXLE0op",
        "outputId": "3a7aeee4-9618-4702-9a88-c7f23e88e146"
      },
      "source": [
        "fill_mask(\"The capital of France is [MASK].\")"
      ],
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.35380667448043823,\n",
              "  'sequence': '[CLS] the capital of france is paris. [SEP]',\n",
              "  'token': 3000,\n",
              "  'token_str': 'paris'},\n",
              " {'score': 0.23379506170749664,\n",
              "  'sequence': '[CLS] the capital of france is strasbourg. [SEP]',\n",
              "  'token': 18104,\n",
              "  'token_str': 'strasbourg'},\n",
              " {'score': 0.08326232433319092,\n",
              "  'sequence': '[CLS] the capital of france is brussels. [SEP]',\n",
              "  'token': 9371,\n",
              "  'token_str': 'brussels'},\n",
              " {'score': 0.06250171363353729,\n",
              "  'sequence': '[CLS] the capital of france is luxembourg. [SEP]',\n",
              "  'token': 10765,\n",
              "  'token_str': 'luxembourg'},\n",
              " {'score': 0.03878603130578995,\n",
              "  'sequence': '[CLS] the capital of france is nice. [SEP]',\n",
              "  'token': 3835,\n",
              "  'token_str': 'nice'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDWGhFsMaUfa"
      },
      "source": [
        "## Uploading the model to hugginface"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLs0lu40RHXU",
        "outputId": "beb1fe9b-7cdc-4e39-ae59-32c12f2ac4c5"
      },
      "source": [
        "!transformers-cli login"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-24 15:12:00.315576: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "\n",
            "        _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "        _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "        _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "        _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "        _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "        \n",
            "Username: lukabor\n",
            "Password: \n",
            "Login successful\n",
            "Your token: owGAinodNDYLgcfcUpolBRHTXjXREYUTuMKUMolKgeWJumtyzPUJnIGyrlUhPwGbDLXNdPpRsGmcwKAwOQrupEiMeLpTHvOQEQAPhensMTRhKnfudyWiPetvUcaXtnBN \n",
            "\n",
            "Your token has been saved to /root/.huggingface/token\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmfJ4ejXRykB",
        "outputId": "7be01106-a7ec-4aa8-99cc-c9ae7cab6792"
      },
      "source": [
        "!transformers-cli repo create europarl-mlm"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-24 15:12:22.205974: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[90mgit version 2.17.1\u001b[0m\n",
            "Error: unknown flag: --version\n",
            "\n",
            "\u001b[90mSorry, no usage text found for \"git-lfs\"\u001b[0m\n",
            "\n",
            "You are about to create \u001b[1mlukabor/europarl-mlm\u001b[0m\n",
            "Proceed? [Y/n] Y\n",
            "\n",
            "Your repo now lives at:\n",
            "  \u001b[1mhttps://huggingface.co/lukabor/europarl-mlm\u001b[0m\n",
            "\n",
            "You can clone it locally with the command below, and commit/push as usual.\n",
            "\n",
            "  git clone https://huggingface.co/lukabor/europarl-mlm\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkhQNCPAR8dO",
        "outputId": "7063b054-3f9f-42af-a3dd-93960af7e9c3"
      },
      "source": [
        "!git clone https://huggingface.co/lukabor/europarl-mlm"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error: Failed to call git rev-parse --git-dir --show-toplevel: \"fatal: not a git repository (or any of the parent directories): .git\\n\"\n",
            "Git LFS initialized.\n",
            "Cloning into 'europarl-mlm'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 3 (delta 0), reused 0 (delta 0)\u001b[K\n",
            "Unpacking objects: 100% (3/3), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xqlvy8r5T9g3",
        "outputId": "0105908e-bee5-4541-cadf-f55915290955"
      },
      "source": [
        "%cd europarl-mlm"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/europarl-mlm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4rZXTwFT_FO",
        "outputId": "b7ca740d-717d-4ba1-db72-41a37ee3e2a9"
      },
      "source": [
        "!git lfs install"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated git hooks.\n",
            "Git LFS initialized.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBNyLPb-SK7i"
      },
      "source": [
        "!cp /content/model/config.json /content/europarl-mlm/\n",
        "!cp /content/model/pytorch_model.bin /content/europarl-mlm/\n",
        "!cp /content/model/special_tokens_map.json /content/europarl-mlm/\n",
        "!cp /content/model/tokenizer_config.json /content/europarl-mlm/\n",
        "!cp /content/model/train_results.txt /content/europarl-mlm/\n",
        "!cp /content/model/training_args.bin /content/europarl-mlm/\n",
        "!cp /content/model/vocab.txt /content/europarl-mlm/"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGIoVhILSr5Q",
        "outputId": "85635f63-27ea-4e4c-9496-a8fb1465c9fa"
      },
      "source": [
        "!git lfs track ."
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tracking \".\"\n",
            "Pattern . matches forbidden file .gitattributes. If you would like to track .gitattributes, modify .gitattributes manually.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq6HXAdnUhiN"
      },
      "source": [
        "!git add .gitattributes"
      ],
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u616PNuRVf0M"
      },
      "source": [
        "!git add ."
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwioc0z2Stlj",
        "outputId": "5ac3a143-71c6-4be6-d4d1-623a6cafde64"
      },
      "source": [
        "!git commit -m 'uloading the model'"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[main 137e53f] uloading the model\n",
            " 6 files changed, 30530 insertions(+)\n",
            " create mode 100644 pytorch_model.bin\n",
            " create mode 100644 special_tokens_map.json\n",
            " create mode 100644 tokenizer_config.json\n",
            " create mode 100644 train_results.txt\n",
            " create mode 100644 training_args.bin\n",
            " create mode 100644 vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmBoVmLCSvXc",
        "outputId": "978e5d75-8b2b-48dc-91b3-0e68ebc0f127"
      },
      "source": [
        ""
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Git LFS: (2 of 2 files) 417.85 MB / 417.85 MB\n",
            "Counting objects: 8, done.\n",
            "Delta compression using up to 2 threads.\n",
            "Compressing objects: 100% (7/7), done.\n",
            "Writing objects: 100% (8/8), 109.05 KiB | 5.19 MiB/s, done.\n",
            "Total 8 (delta 0), reused 0 (delta 0)\n",
            "To https://huggingface.co/lukabor/europarl-mlm\n",
            "   d732aa4..137e53f  main -> main\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}